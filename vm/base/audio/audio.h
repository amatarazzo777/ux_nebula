/*
 * This file is part of the ux_nebula distribution
 * (https://github.com/amatarazzo777/ux_gui_stream).
 * Copyright (c) 2023 Anthony Matarazzo.
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, version 3.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
 * General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <http://www.gnu.org/licenses/>.
 */

/*
The ability to create an emmersive audio foundation that will be reused
does require many functions of design plan. To list the types of features
present in context, musical structure,  DAW capabilities and sound for stereo
and surround has a known series of primatives in architecture. Planning
implementation from existing linux code has its platform awareness and
downfalls. The positives are there is a working system that can be studied.
Audio mixing, chain loading. The sound card provisions, are not direct but
catalog a mountain of cards to support. Changing thre port status, etc. The
ability to link directly the sound card, as a function system compoent, exists
in the same light as video. Displaying text, or even playing notes. Extensive
amounts of code exists to form ideas. As a system linux resource, the main
instance of the driver for sound should be programmed  from for sound.

Supporting multiple drivers for sound, sound boards, and other aspects
becomes a tedious model for some technology. Yet sound remains
embarked upon as a select programming protocol. A layer will exist,
and providing the planned mixing capabilities inline. As data
structures most types of processing can be compiled, and reformed.
The effectiveness of BitWig does seem enticing. Yet internally,
too much complexity exists. Because it is a complete application.
It has to provide support for many formats. Using
LLVM in conjunction with language and UI can be advanced.

Sound production in digital form is simply a series of numbers, integer even.
16bit audio is a 16bit integer with a range beteen -32767 and +32767. This
integer is expects to vary over time to create wave patterns. This number
directly changes the position of the woofer. example, the square wave is simply
both range numbers repeated a certain number of times.Since the information is
consumed at a particular rate, it must be continually produced to change.

Some interesting aspects of knowledge to think about is how does a system wide
floating point representation change this except during driver communication.
Does it change the quality, and errors?


Connecting these objects to the generic os abstract class in desireable.
Keeping the code base with few audio codes. .flac seems good for losless, and
mp3 higher bitrate. mp3 is a licenses form, while ogg vorbis has a public domain
version. Perhaps go with ogg source.


The possiblity to control music daw structures for tracks, midi playback, and
rendering mixdown. Automating some types of signal processing for balancing is
useful.

The sound system at the OS system layer at the OS layer should have the
capability to mix audio in chains, provide this service to multiple clients
perhaps, yet the effects system dsp elements, should transpire running per
application process space, or on a different ring.

The necessity of object oriented design for a language specific for
dsp plugins is the most effective. As a base, the plugin has some distict input
and output functions.Providing for UI description is a better decision at
another software layer. while the audio object with states and memory be applied
as a specific general purpose programming language. Function interface visitor
prototypes for calling events. Midi vs Audio. A sampler with velocity mixture,
ADSR. Enevelope and automation. Rates, osculators, timing functions, noises. FM,
wave form producing language with algorithic editing of chunk audio. Live verses
real time.


Text to speech engines are prized. I gather investments is in more dynamic
modeling of the voice and pronouncements. Most, even great sounding, seem in
effective. Yet often when providing such obsecurities to voice, it may make them
un intelligellible because of the amount of variance. The capability to be
enticed is better, as a game media machine, voice model data is often within the
first layer of text described. The utterance. Currently some data, or in the
past has been included which is basic audio of a voice. Rules ar formed for
building of the sound and how to choose the correct proceeding, previous
etterance as well as control rate. These basic principles offer approximations
combined with sophicated audio blending. Yet often the process, as an object
oriented one, that includes more input from the dsp  to create signals can
provite more control over production. Yet often more parameters over tonal
control production with a basic signal is possible to control the sub elements
of it as a time function. essentially the parameters of the speaking context is
passed through as a data stream. By using the current array of utterance
selection and rule base, using perhaps a type of voxel model to produce
envelopes within the utterance using HD stretch and pitch alterations may
increase the approximated system and provide more emotional controls per sample
set. Ultimately the concepts are difficult without advanced knowledge. Systems
that use machine learning perhaps are utilizing a set where fragments of known
language spoken bits are had. Where utterance appear within the text being read,
the type of word and the brush of humaness at that point in reading aloud to the
micrphone.

Providing models and pitch attenuation with audio additions that mingle with
types of speach patterns can ring moods. Creature speech is much easer with this
type of flexibility in speech production. The engine research is important, yet
there are easier methods for content creation using team work. If voices are
place, providing a type of character in game, a person can be the voice talent
chosen and functionally train the model for situation as a voice actor. Perhaps
a type of professional camera and microphone integrated could solve the input
mechanism. Providing appropiate light and color saturated dies to the lips for
shape recognition make it easier, very reactive and a simple technology to
implement. Imagine how fast and found some specifications are for in game dialog
control. As a focus, organic content is a separation from qualities.

Providing types of work in this nature is dynamic, programmers yell for
different reasons other than meeting the giant fifteen foot tentical monster. To
see the captive moments and that type of dynamic spice to what people would be
doing in a FOV, stumbling, surprise and per frame as independent actors their
voice models meeting requirements of the moment. That last bit, when the
tentical sweeps around and the tts engine inspects "eeuh, get ahuuh the
key<done>" and then the creature eats the character. Each tenticle with eyes and
an organic weapon, such as acid thorns.

The true sense of voice identified on a business scale has been identified.
People enjoy knowledge of a variance. On telephony applications it gives an
indication as to what can happen,since you are at a prompt system, drink a sip
of coffee before looking at the keypad on the phone. A bot system can be in
place for example when the boss is called to hear the complaint using a differnt
voice model perhaps is not functionalty as an upgrade path for this type of
functionalty. Yet the ability to produce types of speech patterns, more
approximated to emotion, with types of base DSP effects and mixups as a sound
editing composition can add to models. The pitch alteration, with perhaps
aftertouch effects acoustically defined for a tonal mix, the extra hisses that
vary, along with the deep voice is a typical human interpreted concept of a type
of voice. The dexterity of the model processing and also using sound knowledge
to provides granular decision making to a style, can be parameterized, and often
course parameters as a main controller are identified for use. The three tier
model while the middle teir is the current implementation of such systems, audio
data and database relational intelligence, the kerning of speech.

The magesty of acting, and other types of forward body actions create a one line
story. You could have a program laugh very loud, but the movements of the mouth
and bones do not match on live entertainment visually. So an accurate system
would be compelled to tie these together, suited for daft play. Games and media
would make the best canidate action for this type. Simply applying that it is
turned off, is a matter of tts engine sound design.

There are many problems to be enticed to solve which are held in other sound
engines. Web oriented. Some source code does exist. Newer methods have been
made. Computer voices that even mimic characterizations. Provided with an input
of like software, can be useful. Most likely not a system level compoent
however. No reason the CPU needs to talk. For application layer, a third party
tts is almost necessary.


Some notible audio can be connected to usb. USB of multiple types and multiple
ports. Lucikly the format is very universal for audio card, and digital
microphones.


Using the generic operating system object, to work with basic audio output
and control the volume. Mixing multiple audio together and stop start.

Timing syncronization can occur using the same event systems used by animation.

These classes attach to the main generic_os_t function implemention of class.

The ability to utilize the 5.1 system arrangement as a music format
can be specific for sound field automation. Stereo has not even been
mastered by some. Providing Video animation of computer graphics as
a musical art without branding video productions.
A type of lava lamp for music, show and play music for projection.
The music stream format has tag formats to support what is known as meta data,
typically to a codec, and transfered to dispatch and timing information
along with other parameters to the selected codec in chain. Perhaps
some parts would control usb devices, or blue tooth robots, lamps
dim at scary points and brighten along with the vibration under the cusion.
A rendering projector output of for format and software rendering control
parameters. Models and textures may be transmitted through the format, as most
existing formats provide such as mp3 and also flac,
Audio waves.



https://xiph.org/ao/
Alsa
libasound
<alsa/asoundlib.h>



*/
namespace viewManager {
enum class sampleRate {

};

/*
 a nice interface to the sub systems that provide
 the musical rendering of notes. There are multiple
 plugins that are fabioluous. Yet sticking with
 a limited set of essentials, while completing
 the chain with top level controls and preset chains.
 The recognition of the General Midid sound design
 and sound font produces multiple audiences.
 Simply the program name is still a hard coded number
 for a panio, the drum kit kick is at a specific note.
 The abaility to adapt these systems is fine. These types
 of decisions can be made independently of the low
 level code that allows an interface to describe musical
 data, as a data structure.

*/
class audio_daw_t {
public:
  class daw_node_t {};

  class PCM_t {
  public:
    std::vector<uint32_t> data;
  };

  class parameter_t {
  public:
    uint16_t id;
    std::variant<uint16_t, std::string, float> setting;
  };

  class time_t : daw_node_t {
  public:
    float t;
  };

  class automation_event_t : daw_node_t {
  public:
    time_t ts;
    time_t te;
    uint8_t id;       // the id of the parameter to modify over time.
    uint8_t function; // the function to use lerp on between ts and te
    uint8_t val; // value setting - 255 is a good range for input devices. Many
                 // keyboard only have just 7,
  };

  class automation_curve_t : daw_node_t {
  public:
    std::list<automation_event_t> events;
  };

  class note_t : daw_node_t {
  public:
    uint8_t velocity;
    uint8_t attack;
    uint8_t release;
    uint8_t sustain;
    uint8_t volume;
    automation_curve_t curve;
    time_t ts;
    time_t te;
  };

  class audio_effect_t : public daw_node_t {
  public:
    std::list<parameter_t> p;
    std::function<void> *fn_pointer(void);
    automation_curve_t curve;
    virtual impose(PCM_t data);
  };

  class eq_t : public audio_effect_t {};
  class compressor_t : public audio_effect_t {};
  class crossover_t : public audio_effect_t {};
  class gate_t : public audio_effect_t {};
  class limiter_t : public audio_effect_t {};
  class chorus_t : public audio_effect_t {};
  class reverb_t : public audio_effect_t {};
  class flanger_t : public audio_effect_t {};
  class phaser_t : public audio_effect_t {};
  class distortion_t : public audio_effect_t {};
  class bit_t : public audio_effect_t {};
  class tube_t : public audio_effect_t {};
  class rotary_t : public audio_effect_t {};
  class delay_t : public audio_effect_t {};
  class stereo_t : public audio_effect_t {};
  class transient_t : public audio_effect_t {};
  class filter_t : public audio_effect_t {};
  class comb_t : public audio_effect_t {};
  class DeEsser_t : public audio_effect_t {};
  class pitch_t : public audio_effect_t {};
  class tremolo_t : public audio_effect_t {};

  class effect_chain_t : public daw_node_t {
    std::list<audio_effect_t> n;
  };

  class instrument_t : public daw_node_t {
  public:
    std::string name;
    std::size_t instrument_id;
    std::list<parameter_t> p;
    std::function<void> *fn_pointer(void);
    play_note(const note_t &n);
  };

  class synthezier_t : public daw_node_t {
    std::list<instrument_t> presets;
  };

  class sample_t : daw_node_t {
  public:
    PCM_t data;
  };

  class sampler_t : public instrument_t, public sample_t, public daw_node_t {
  public:
  };

  class pattern_t : public daw_node_t {
  public:
    std::list<note_t> notes;
    std::list<instrument_t> instrument_chain;
    std::vector<uint32_t> PCM;
    automation_curve_t curve;

    time_t length;
  };

  class track_t : daw_node_t {
  public:
    std::list<note_t> notes;
    std::list<instrument_t> instrument_chain;
    std::vector<uint32_t> PCM;
  };

  class timeline_t {
  public:
    std::list<track_t> tracks;
  };

public:
  void render();
  void play();
  void record();
  time_t start;
  time_t end;
  float tempo;
  float measure;
  float swing;
  float clock;
};

class audio_t {
public:
  audio_t() = 0;
  ;
  audio_t(std::shared_ptr<os_interface_manager_t> _os) : m_os(_os){};
  std::shared_ptr<os_interface_manager_t> m_os;

  void connect(std::size_t _size);
  void disconnect(std::size_t _size);

  void mix_data(unsigned char **, std::size_t);
  void head(unsigned char **);
  void tail(unsigned char **);

  // callback when buffer need filling. Only the data
  // parts of unused areas of circular queue.
  // a buffer could be a connection the the audio
  // codec system. The memory is alsa and also shared memory buffer.
  // how to combine them into one seems nice. At this layer,
  // it should be a compositie audio signal inside the class.
  // Perhaps one per application requiring it could
  // begin to become tedious in memory usage. Usually
  // the mix down buffers are smaller for driver interfaces.

  void virtual fn_buffer() = 0;
  void play(unsigned char *, unsigned char *);
  void stop();
  void pause();
  void setVolume();
}
} // namespace viewManager